{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Utility functions and general imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import scipy as scpy\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from scipy import optimize as opt\n",
    "from sklearn import decomposition\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Creating the functions used throughout the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Defining utility functions for reading images and data in\n",
    "def show_image(array, label):\n",
    "    im = Image.fromarray(array)\n",
    "    print(\"Item of label={}\".format(label))\n",
    "    return(imshow(im))\n",
    "\n",
    "def construct_conf_mat(actual_y, predicted_prob, pos_class_label, thresh=0.5):\n",
    "    fp, tp, fn, tn = 0, 0, 0, 0\n",
    "    for i in range(actual_y.shape[0]):\n",
    "        if predicted_prob[i] >= thresh:\n",
    "            if actual_y[i] == pos_class_label:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "        else:\n",
    "            if actual_y[i] == pos_class_label:\n",
    "                fn += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "    return fp, tp, fn, tn\n",
    "    \n",
    "    \n",
    "def compress_project(percent, data_to_compress):\n",
    "\n",
    "# Find the principal components that explain 99% of the data\n",
    "    pca = decomposition.PCA(percent)\n",
    "\n",
    "    #Run PCA on normalized image data\n",
    "    lower_dim_data = pca.fit_transform(data_to_compress)    \n",
    "    print('Shape of lower dimension image {}:'.format(lower_dim_data.shape))\n",
    "\n",
    "    #Project lower dimension data onto original features\n",
    "    approximation = pca.inverse_transform(lower_dim_data)\n",
    "    #approximation = pca.inverse_transform(lower_dim_data)\n",
    "        \n",
    "    #Reshape approximation and X_norm to 784*28*28 to display images\n",
    "    approximation_reshaped = approximation.reshape(-1,784)\n",
    "    print('Shape of reconstructed image {}:'.format(approximation_reshaped.shape))   \n",
    "    print( 'Is Original close to Resized? ', np.allclose(data_to_compress, approximation) )\n",
    "    \n",
    "    return (approximation, lower_dim_data) \n",
    "\n",
    "\n",
    "def shape_matrix_process(split_percent):\n",
    "    \n",
    "            \n",
    "        # Read the source files in\n",
    "        data, label, test_data, test_label = read_data_in(desktop_or_laptop='d')\n",
    "\n",
    "        # Creating a fair split of training and validation data for the \n",
    "        # training model to be trained on\n",
    "        indices = range(data.shape[0])\n",
    "        classes = np.unique(label)\n",
    "        \n",
    "        training_records = int(split_percent * data.shape[0])        \n",
    "        \n",
    "        # Get the records that are part of the indices declared above\n",
    "        train_data = data[:training_records]\n",
    "        validate_data = data[training_records:]        \n",
    "        \n",
    "        # Get the labels that are part of the indices declared above\n",
    "        label_train = label[:training_records]\n",
    "        label_validate = label[training_records:]        \n",
    "        \n",
    "        \n",
    "        get_class_data = np.vectorize(lambda arr: label==i )\n",
    "                        \n",
    "        \n",
    "        for i in range(len(np.unique(y_test))):\n",
    "            for j in range(len(np.unique(y_test))):\n",
    "                text = ax.text(j, i, cm[i, j],\n",
    "                       ha=\"center\", va=\"center\", color=\"k\")\n",
    "        \n",
    "        for i in classes:\n",
    "            print(\"Pixel histogram prior to feature scaling : \".format(classes[i]))\n",
    "            print(i)\n",
    "        \n",
    "            d=[]\n",
    "            d = data[get_class_data(i)]\n",
    "            h = plt.hist(d[i].ravel())\n",
    "            plt.show()\n",
    "\n",
    "        X_train = train_data.reshape(-1, 784)\n",
    "        X_validate = validate_data.reshape(-1, 784)        \n",
    "        \n",
    "        y_train = np.ravel(label_train)\n",
    "        y_validate = np.ravel(label_validate)    \n",
    "\n",
    "                \n",
    "        ##########################################################################  \n",
    "        # MINMAXSCALER IS THE ONLY SCALER FOUND TO RELIABLY RESULT IN CONVERGENCE*\n",
    "        # HOWEVER NEEDS FURTHER TESTING =========================================#\n",
    "        ##########################################################################\n",
    "\n",
    "        # Fit & transform to our training data set\n",
    "        X_train = X_train.astype('float32')\n",
    "        X_train = normalise(X_train)\n",
    "        \n",
    "        \n",
    "        # Change our X_Validate data set to float 32 to \n",
    "        \n",
    "        X_validate = X_validate.astype('float32')\n",
    "        X_validate = normalise(X_validate)\n",
    "\n",
    "        print('Total number of records in X train: {}'.format(X_train.shape[0]))\n",
    "        print('Total number of records in y train: {}'.format(y_train.shape[0]))\n",
    "        print('Total number of features in X train: {}'.format(X_train.shape[1]))\n",
    "\n",
    "        print('Total number of records in X train (valdation set): {}'.format(X_validate.shape[0]))\n",
    "        print('Total number of features in X train (validation set): {}'.format(X_validate.shape[1]))\n",
    "        \n",
    "       \n",
    "        ########################\n",
    "        # Now to transform test#\n",
    "        ########################\n",
    "        \n",
    "        X_test = test_data[:2000]        \n",
    "        X_test = test_data.reshape(-1,784)\n",
    "        y_test = test_label [:2000]       \n",
    "        X_test = X_test.astype('float32')\n",
    "            \n",
    "        ##########################################################################  \n",
    "        # MINMAXSCALER IS THE ONLY SCALER FOUND TO RELIABLY RESULT IN CONVERGENCE*\n",
    "        # HOWEVER NEEDS FURTHER TESTING =========================================#\n",
    "        ##########################################################################\n",
    "\n",
    "   \n",
    "        X_test = normalise(X_test)\n",
    "                                          \n",
    "        print('Total number of records in X test: {}'.format(X_test.shape[0]))\n",
    "        print('Total number of features in X test: {}'.format(X_test.shape[1]))\n",
    "        \n",
    "        return(X_train,X_validate,X_test,y_train,y_validate,y_test)\n",
    "    \n",
    "    \n",
    "# Import the files in from python h5 format\n",
    "\n",
    "\n",
    "def read_data_in(desktop_or_laptop='d'):\n",
    "    \n",
    "    if desktop_or_laptop == 'l':\n",
    "\n",
    "        ## Dan's Mac folder location - NEEDS CHANGING\n",
    "        with h5py.File('../Project1/data/images_training.h5','r') as H:\n",
    "            data = np.copy(H['data'])\n",
    "        with h5py.File('../Project1/data/labels_training.h5','r') as H:\n",
    "            label = np.copy(H['label'])\n",
    "\n",
    "        with h5py.File('../Project1/data/images_testing.h5','r') as H:\n",
    "            data_test = np.copy(H['data'])\n",
    "        with h5py.File('../Project1/data/labels_testing_2000.h5','r') as H:\n",
    "            label_test = np.copy(H['label'])    \n",
    "\n",
    "\n",
    "    else:\n",
    "        ## Dan's desktop folder location - NEEDS CHANGING\n",
    "        with h5py.File('../../Input/images_training.h5','r') as H:\n",
    "            data = np.copy(H['data'])\n",
    "        with h5py.File('../../Input/labels_training.h5','r') as H:\n",
    "            label = np.copy(H['label'])\n",
    "        with h5py.File('../../Input/images_testing.h5','r') as H:\n",
    "            data_test = np.copy(H['data'])\n",
    "\n",
    "        with h5py.File('../../Input/labels_testing_2000.h5','r') as H:\n",
    "            label_test = np.copy(H['label'])     \n",
    "    \n",
    "    return(data, label, data_test, label_test)\n",
    "\n",
    "def add_intercept(X_mat,y_mat):\n",
    "    m = len(y_mat)\n",
    "    ones = np.ones((m,1))\n",
    "    \n",
    "    X = np.concatenate((ones,X_mat),axis=1)\n",
    "    m,n = X.shape\n",
    "    print(X.shape)\n",
    "    return(X,n)\n",
    "\n",
    "# Is the matrix symmetric?\n",
    "def is_symmetric(X, tolerance = 1e-9):\n",
    "    return(np.allclose(X,X.T, atol=tolerance))\n",
    "\n",
    "def get_minmax(X):\n",
    "    minmax = list()\n",
    "    for i in range(len(X[0])):\n",
    "        column_values = [row[i] for row in X]\n",
    "        mn = min(column_values)\n",
    "        mx = max(column_values)\n",
    "        minmax.append([mn,mx])\n",
    "    return(minmax)\n",
    "\n",
    "def normalise(X):\n",
    "    #print(X_train.shape)\n",
    "    X_scaled = (X - np.min(X,axis=0)) / (np.max(X, axis=0) - (np.min(X,axis=0) ))\n",
    "    return(X_scaled)\n",
    "\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self,  max_iter, intercept, k, lmbda ):\n",
    "        #self.lr = lr\n",
    "        self.max_iter = max_iter\n",
    "        self.intercept = intercept\n",
    "        self.k = k\n",
    "        self.lmbda = lmbda\n",
    "        print (\"Logistic Regression model initialised with Max iterations = {}, K (classes) = {}, lmbda (Regularisation parameter) ={}\".format(self.max_iter,self.k, self.lmbda))\n",
    "    \n",
    "    def add_intercept(self, X_mat,y_mat):\n",
    "        m = len(y_mat)\n",
    "        ones = np.ones((m,1))\n",
    "\n",
    "        X_mat = np.concatenate((ones,X_mat),axis=1)\n",
    "        m_shape,n_shape = X_mat.shape\n",
    "        #print(\"n_shape: {}\".format(n_shape))\n",
    "        #print(\"X_mat.shape: {}\".format(X_mat.shape))\n",
    "        \n",
    "        return(X_mat,n_shape)\n",
    "    \n",
    "    # Defining the sigmoid function required in LR\n",
    "    def __sigmoid(self, z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def __cost(self, theta, X_arr, y_arr):\n",
    "        predictions = self.__sigmoid(X_arr @ theta)\n",
    "        predictions[predictions == 1] = 0.999 # log(1)=0 causes error in division\n",
    "        error = -y_arr * np.log(predictions) - (1 - y_arr) * np.log(1 - predictions);\n",
    "        return sum(error) / len(y_arr);\n",
    "    \n",
    "    # Vectorised cost function\n",
    "    def __costFunctionReg(self,theta, X, y, lmbda):\n",
    "        m = len(y)\n",
    "        temp1 = np.multiply(y, np.log(self.__sigmoid(np.dot(X, theta))))\n",
    "        temp2 = np.multiply(1-y, np.log(1-self.__sigmoid(np.dot(X, theta))))  \n",
    "        #print(np.sum(temp1 + temp2) / (-m) + np.sum(theta[1:]**2) * lmbda / (2*m))\n",
    "        return np.sum(temp1 + temp2) / (-m) + np.sum(theta[1:]**2) * lmbda / (2*m)\n",
    "    \n",
    "    \n",
    "    def fit(self, X_mat,max_iter, y_mat,n, full_outp):\n",
    "                        \n",
    "        theta = np.zeros((k,n)) #inital parameters\n",
    "        cost_values = {}\n",
    "        \n",
    "        \n",
    "        print(\"                                            \")\n",
    "        print(\"############################################\")\n",
    "        print(\"### Gradient Descent Optimisation beginning\")\n",
    "        print(\"############################################\")\n",
    "        for i in range(k):\n",
    "            label_class = i if i else 0\n",
    "            print('Class {} being optimised'.format(i))\n",
    "            #cost0 = cost(theta, X, (y==label_class.flatten()))\n",
    "            #cost[i] = cost(theta[i], X,y)\n",
    "            #print('Cost for Class {}:'.format(cost[i]))               \n",
    "            #ValueError: cannot copy sequence with size 5 to array axis with dimension 785\n",
    "            theta[i] = opt.fmin_cg(f = self.__costFunctionReg, x0 = theta[i].flatten(), gtol = 1e-03, fprime = self.__gradient_reg_vectorised, args = (X_mat,(y_mat == label_class).flatten(), self.lmbda),maxiter= max_iter, disp = True, full_output = full_outp)                        \n",
    "            #cost = {i,}\n",
    "            #theta[i] = opt.fmin_cg(f = costFunctionReg, x0 = theta[i].flatten(),  fprime = gradRegularization, args = (X_mat,(y == label_class).flatten(), lmbda), maxiter = 200, disp = True)    \n",
    "            \n",
    "        print(\"###########################################\")\n",
    "        print(\"### Gradient Descent Optimisation finished\")\n",
    "        print(\"###########################################\")\n",
    "            #if full_outp == 1:\n",
    "            #    return(theta, fopt, func_calls, grad_calls, warnflag)\n",
    "            #else:\n",
    "            #    return(theta)\n",
    "        return(theta)\n",
    "    \n",
    "      \n",
    "    \n",
    "    # Vectorised gradient\n",
    "    def __gradient_reg_vectorised(self, theta, X_arr, y_arr, lmbda):\n",
    "        m = len(y_arr)\n",
    "        temp = self.__sigmoid(np.dot(X_arr, theta)) - y_arr\n",
    "        temp = np.dot(temp.T, X_arr).T / m + theta * lmbda / m\n",
    "        temp[0] = temp[0] - theta[0] * lmbda / m\n",
    "        return temp\n",
    "    \n",
    "    # Vectorised cost function\n",
    "    def __cost_func_vectorised(self,theta, X_arr, y_arr, lmbda):\n",
    "        m = len(y_arr)\n",
    "        temp_weight1 = np.multiply(y_arr, np.log(self.__sigmoid(np.dot(X_arr, theta))))\n",
    "        temp_weight2 = np.multiply(1-y_arr, np.log(1-self.__sigmoid(np.dot(X_arr, theta))))\n",
    "        return np.sum(temp_weight1 + temp_weight2) / (-m) + np.sum(theta[1:]**2) * self.lmbda / (2*m)\n",
    "    \n",
    "    \n",
    "    def predict(self,X,y,theta):    \n",
    "        preds = []\n",
    "        preds = np.argmax(X @ theta.T, axis = 1)\n",
    "        preds = [e if e else 0 for e in preds]\n",
    "        average_pred = np.mean(preds == y.flatten()) * 100        \n",
    "        return(preds, average_pred)\n",
    "        #return(average_pred)\n",
    "    \n",
    "    def predict_prob(self,X, theta):\n",
    "        return (self.__sigmoid(np.dot(X),theta.T))\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1f0825eba20>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel histogram prior to feature scaling : 0 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([23.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  2.,\n",
       "          0.,  1.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([21.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  1.,  0.,  0.,  2.,  3.,  0.]),\n",
       "  array([20.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  6.,  1.,  0.,  0.]),\n",
       "  array([20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  1.,  1.,  1.,  3.,  1.,  1.]),\n",
       "  array([18.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  5.,  1.,  1.,  1.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 3., 1., 5., 3., 3., 2.,\n",
       "         2., 2., 3.]),\n",
       "  array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  2.,  0.,  0.,\n",
       "          0.,  1.,  2.,  3., 15.,  3.,  0.]),\n",
       "  array([ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  1.,  0.,  0.,  2.,  0.,\n",
       "          0.,  2.,  5., 11.,  3.,  0.,  2.]),\n",
       "  array([ 0.,  0.,  0.,  0.,  1.,  1.,  0.,  1.,  0.,  0.,  0.,  1.,  1.,\n",
       "          1.,  3.,  3., 14.,  1.,  0.,  1.]),\n",
       "  array([ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  1.,  2.,  1.,\n",
       "          2.,  1.,  2., 12.,  1.,  3.,  1.]),\n",
       "  array([ 0.,  0.,  0.,  0.,  2.,  0.,  0.,  0.,  1.,  0.,  1.,  3.,  0.,\n",
       "          0.,  1.,  0., 14.,  3.,  1.,  2.]),\n",
       "  array([ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  0.,  2.,\n",
       "          1.,  4.,  1., 14.,  0.,  0.,  2.]),\n",
       "  array([ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  3.,  0.,  2.,  1.,\n",
       "          0.,  3.,  1., 13.,  1.,  0.,  2.]),\n",
       "  array([ 0.,  0.,  0.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
       "          3.,  0.,  0., 14.,  1.,  1.,  4.]),\n",
       "  array([ 0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  1.,  2.,  2.,\n",
       "          0.,  2.,  1., 12.,  1.,  4.,  1.]),\n",
       "  array([ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  2.,  0.,  0.,\n",
       "          3.,  3.,  2., 11.,  3.,  1.,  1.]),\n",
       "  array([ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  2.,  0.,  0.,  1.,  1.,  0.,\n",
       "          1.,  1.,  5., 11.,  2.,  2.,  1.]),\n",
       "  array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  1.,  0.,\n",
       "          0.,  1.,  0.,  5., 16.,  1.,  1.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 3., 6., 0., 1., 4., 6.,\n",
       "         2., 3., 1.]),\n",
       "  array([18.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  1.,  2.,  3.,  2.,  0.]),\n",
       "  array([19.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  2.,  5.,  1.,  0.]),\n",
       "  array([19.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  3.,  2.,  2.,  0.]),\n",
       "  array([22.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          1.,  0.,  0.,  1.,  1.,  1.,  2.]),\n",
       "  array([24.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  1.,  0.,  0.,  1.,  0.,  0.]),\n",
       "  array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.])],\n",
       " array([  0.  ,  12.75,  25.5 ,  38.25,  51.  ,  63.75,  76.5 ,  89.25,\n",
       "        102.  , 114.75, 127.5 , 140.25, 153.  , 165.75, 178.5 , 191.25,\n",
       "        204.  , 216.75, 229.5 , 242.25, 255.  ]),\n",
       " <a list of 28 Lists of Patches objects>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1f0828c8e48>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel histogram prior to feature scaling : 1 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 3., 3., 9., 3., 1., 2.,\n",
       "         3., 0., 0.]),\n",
       "  array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  4., 14.,  7.,  3.]),\n",
       "  array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0., 11., 12.,  5.,  0.]),\n",
       "  array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          1.,  0.,  0.,  4., 10., 13.,  0.]),\n",
       "  array([5., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 3.,\n",
       "         5., 4., 1.]),\n",
       "  array([19.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  2.,  3.,  1.,  0.]),\n",
       "  array([6., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 4.,\n",
       "         7., 2., 0.]),\n",
       "  array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "          0.,  2.,  3., 11., 10.,  1.,  0.]),\n",
       "  array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  1., 11., 14.,  1.,  1.,  0.]),\n",
       "  array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0., 10., 12.,  4.,  1.,  1.]),\n",
       "  array([ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,  2.,  0.,\n",
       "          2.,  9., 13.,  0.,  0.,  0.,  0.]),\n",
       "  array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.])],\n",
       " array([  0.  ,  12.75,  25.5 ,  38.25,  51.  ,  63.75,  76.5 ,  89.25,\n",
       "        102.  , 114.75, 127.5 , 140.25, 153.  , 165.75, 178.5 , 191.25,\n",
       "        204.  , 216.75, 229.5 , 242.25, 255.  ]),\n",
       " <a list of 28 Lists of Patches objects>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1f086c78358>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel histogram prior to feature scaling : 2 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([12.,  1.,  0.,  1.,  0.,  2.,  0.,  2.,  1.,  0.,  2.,  2.,  3.,\n",
       "          2.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([4., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 2., 0., 1., 1., 6.,\n",
       "         3., 7., 2.]),\n",
       "  array([2., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 9.,\n",
       "         7., 2., 6.]),\n",
       "  array([1., 0., 1., 0., 1., 0., 0., 2., 2., 0., 2., 1., 0., 0., 1., 2., 2.,\n",
       "         6., 4., 3.]),\n",
       "  array([5., 0., 0., 0., 0., 0., 0., 2., 2., 2., 1., 1., 0., 1., 0., 3., 3.,\n",
       "         3., 2., 3.]),\n",
       "  array([ 4.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
       "          0.,  0., 12.,  3.,  2.,  5.,  1.]),\n",
       "  array([ 3.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,\n",
       "          0.,  0.,  8., 14.,  1.,  0.,  0.]),\n",
       "  array([ 3.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  7., 15.,  2.,  0.,  0.]),\n",
       "  array([ 3.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,\n",
       "          0.,  0.,  6., 15.,  1.,  0.,  1.]),\n",
       "  array([ 3.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,\n",
       "          0.,  0.,  5., 16.,  1.,  0.,  1.]),\n",
       "  array([ 3.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  5., 16.,  1.,  0.,  1.]),\n",
       "  array([ 3.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,\n",
       "          0.,  0.,  6., 15.,  1.,  0.,  1.]),\n",
       "  array([ 3.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,\n",
       "          0.,  0.,  6., 15.,  0.,  2.,  0.]),\n",
       "  array([ 3.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  7., 15.,  1.,  1.,  0.]),\n",
       "  array([ 3.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "          0.,  0.,  8., 13.,  2.,  0.,  0.]),\n",
       "  array([ 4.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0., 12.,  3.,  3.,  3.,  2.]),\n",
       "  array([5., 0., 0., 0., 1., 0., 0., 0., 1., 2., 0., 4., 1., 0., 0., 3., 4.,\n",
       "         1., 2., 4.]),\n",
       "  array([1., 2., 0., 1., 2., 1., 2., 0., 0., 0., 0., 0., 1., 0., 1., 3., 3.,\n",
       "         4., 6., 1.]),\n",
       "  array([2., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 7.,\n",
       "         6., 5., 6.]),\n",
       "  array([4., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 2., 0., 0., 7.,\n",
       "         2., 6., 5.]),\n",
       "  array([12.,  1.,  0.,  1.,  1.,  1.,  2.,  2.,  1.,  2.,  1.,  2.,  1.,\n",
       "          1.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.])],\n",
       " array([  0.  ,  12.75,  25.5 ,  38.25,  51.  ,  63.75,  76.5 ,  89.25,\n",
       "        102.  , 114.75, 127.5 , 140.25, 153.  , 165.75, 178.5 , 191.25,\n",
       "        204.  , 216.75, 229.5 , 242.25, 255.  ]),\n",
       " <a list of 28 Lists of Patches objects>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1f0877870f0>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel histogram prior to feature scaling : 3 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([18.,  1.,  0.,  0.,  1.,  0.,  1.,  1.,  2.,  1.,  0.,  0.,  1.,\n",
       "          0.,  1.,  1.,  0.,  0.,  0.,  0.]),\n",
       "  array([11.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,\n",
       "          2.,  2.,  3.,  2.,  2.,  0.,  3.]),\n",
       "  array([2., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 5., 2., 2., 0., 5.,\n",
       "         2., 3., 0.]),\n",
       "  array([1., 0., 2., 1., 0., 1., 1., 0., 2., 0., 0., 1., 0., 2., 4., 4., 2.,\n",
       "         3., 2., 2.]),\n",
       "  array([0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 3., 7., 4., 6.,\n",
       "         2., 2., 1.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 3., 9., 7., 6.,\n",
       "         1., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 3., 4., 1., 6.,\n",
       "         7., 4., 1.]),\n",
       "  array([ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,\n",
       "          1.,  4.,  4.,  2.,  1.,  2., 11.]),\n",
       "  array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,\n",
       "          2.,  5.,  7., 10.,  1.,  0.,  1.]),\n",
       "  array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  2.,  0.,  0.,\n",
       "          4.,  4., 12.,  4.,  1.,  0.,  1.]),\n",
       "  array([ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "          1.,  6.,  7., 10.,  2.,  0.,  0.]),\n",
       "  array([0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 2., 3., 4., 4., 6.,\n",
       "         3., 3., 1.]),\n",
       "  array([1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 4., 5., 6.,\n",
       "         3., 0., 3.]),\n",
       "  array([ 1.,  0.,  0.,  0.,  0.,  1.,  3.,  1.,  1.,  2.,  0.,  1.,  0.,\n",
       "          1.,  2.,  2., 11.,  2.,  0.,  0.]),\n",
       "  array([9., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 2.,\n",
       "         4., 5., 0.]),\n",
       "  array([15.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  2.,\n",
       "          0.,  1.,  2.,  2.,  2.,  0.,  1.]),\n",
       "  array([22.,  2.,  0.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.])],\n",
       " array([  0.  ,  12.75,  25.5 ,  38.25,  51.  ,  63.75,  76.5 ,  89.25,\n",
       "        102.  , 114.75, 127.5 , 140.25, 153.  , 165.75, 178.5 , 191.25,\n",
       "        204.  , 216.75, 229.5 , 242.25, 255.  ]),\n",
       " <a list of 28 Lists of Patches objects>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1f08325ee10>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel histogram prior to feature scaling : 4 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([15.,  0.,  0.,  2.,  0.,  1.,  1.,  1.,  2.,  2.,  3.,  0.,  0.,\n",
       "          1.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([8., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 6., 4., 5.,\n",
       "         0., 1., 0.]),\n",
       "  array([6., 0., 0., 1., 0., 0., 0., 0., 0., 0., 2., 2., 2., 4., 8., 2., 1.,\n",
       "         0., 0., 0.]),\n",
       "  array([5., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 4., 0., 4.,\n",
       "         2., 5., 6.]),\n",
       "  array([3., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 4., 5., 6., 3., 3.,\n",
       "         1., 1., 0.]),\n",
       "  array([2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 5., 5., 7., 2., 5.,\n",
       "         1., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 6., 7., 3., 4., 5.,\n",
       "         0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 6., 4., 3., 2., 3., 4.,\n",
       "         3., 0., 1.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 2., 2., 0., 8., 2., 4., 4., 1., 0., 0.,\n",
       "         3., 0., 1.]),\n",
       "  array([ 0.,  0.,  0.,  0.,  0.,  0.,  2.,  0.,  1.,  0.,  0.,  1.,  0.,\n",
       "          0.,  1.,  2.,  9., 10.,  2.,  0.]),\n",
       "  array([0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 2., 9., 3., 4., 4., 0.,\n",
       "         2., 2., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 2., 2., 3., 2., 3., 3., 0., 1., 5., 5., 0.,\n",
       "         0., 1., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 5., 3., 3., 5., 1., 1., 3., 4., 1.,\n",
       "         1., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 5., 7., 3., 4., 1., 3., 2.,\n",
       "         1., 0., 0.]),\n",
       "  array([1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 3., 7., 3., 2., 7., 0., 2.,\n",
       "         1., 0., 0.]),\n",
       "  array([4., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 3., 1., 1., 5., 4., 4.,\n",
       "         3., 0., 0.]),\n",
       "  array([6., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 5., 4., 4., 5., 1., 1.,\n",
       "         0., 0., 0.]),\n",
       "  array([7., 0., 0., 0., 1., 0., 0., 0., 1., 1., 2., 2., 4., 2., 1., 4., 0.,\n",
       "         2., 1., 0.]),\n",
       "  array([12.,  1.,  0.,  2.,  0.,  1.,  2.,  1.,  3.,  1.,  2.,  3.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.])],\n",
       " array([  0. ,  12.7,  25.4,  38.1,  50.8,  63.5,  76.2,  88.9, 101.6,\n",
       "        114.3, 127. , 139.7, 152.4, 165.1, 177.8, 190.5, 203.2, 215.9,\n",
       "        228.6, 241.3, 254. ]),\n",
       " <a list of 28 Lists of Patches objects>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1f08325c748>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel histogram prior to feature scaling : 5 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([array([26.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([26.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  1.,  1.,  0.]),\n",
       "  array([25.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  1.,  0.,  0.,  0.,  1.,  0.]),\n",
       "  array([25.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  1.,  0.,  0.,  0.,  1.,  0.]),\n",
       "  array([24.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  1.,  0.,  1.,  0.]),\n",
       "  array([21.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,\n",
       "          1.,  0.,  0.,  0.,  1.,  0.,  2.]),\n",
       "  array([20.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  2.,  0.,\n",
       "          0.,  1.,  0.,  1.,  0.,  0.,  3.]),\n",
       "  array([17.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  2.,  1.,  0.,\n",
       "          1.,  1.,  2.,  0.,  1.,  1.,  1.]),\n",
       "  array([17.,  1.,  0.,  0.,  0.,  0.,  2.,  0.,  0.,  1.,  0.,  1.,  0.,\n",
       "          1.,  2.,  1.,  0.,  0.,  0.,  2.]),\n",
       "  array([16.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  2.,  0.,  0.,  0.,  1.,\n",
       "          0.,  1.,  2.,  0.,  1.,  2.,  1.]),\n",
       "  array([14.,  1.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  1.,  1.,  0.,  1.,\n",
       "          0.,  1.,  1.,  0.,  3.,  0.,  3.]),\n",
       "  array([17.,  0.,  1.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  1.,  1.,  2.,  2.,  0.,  2.]),\n",
       "  array([18.,  0.,  0.,  1.,  0.,  0.,  0.,  2.,  0.,  0.,  2.,  0.,  1.,\n",
       "          0.,  0.,  0.,  0.,  0.,  3.,  1.]),\n",
       "  array([19.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "          0.,  2.,  2.,  1.,  1.,  0.,  2.]),\n",
       "  array([16.,  0.,  1.,  1.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  2.,\n",
       "          0.,  1.,  1.,  1.,  0.,  1.,  2.]),\n",
       "  array([16.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  2.,  1.,  1.,  0.,\n",
       "          0.,  2.,  0.,  0.,  1.,  1.,  3.]),\n",
       "  array([15.,  0.,  1.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  1.,  1.,  1.,\n",
       "          2.,  1.,  0.,  0.,  1.,  1.,  2.]),\n",
       "  array([17.,  0.,  1.,  0.,  1.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,\n",
       "          2.,  0.,  0.,  0.,  1.,  2.,  2.]),\n",
       "  array([19.,  1.,  0.,  0.,  3.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
       "          0.,  0.,  1.,  0.,  1.,  1.,  1.]),\n",
       "  array([21.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,\n",
       "          0.,  0.,  0.,  0.,  2.,  0.,  1.]),\n",
       "  array([22.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "          1.,  0.,  1.,  1.,  1.,  0.,  1.]),\n",
       "  array([22.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
       "          1.,  1.,  1.,  0.,  0.,  0.,  2.]),\n",
       "  array([21.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,\n",
       "          0.,  0.,  0.,  1.,  0.,  0.,  3.]),\n",
       "  array([16.,  0.,  1.,  1.,  0.,  1.,  0.,  2.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  1.,  1.,  0.,  2.,  3.]),\n",
       "  array([4., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 2., 4., 3., 1., 0., 2.,\n",
       "         3., 3., 4.]),\n",
       "  array([5., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 5., 3., 1., 1.,\n",
       "         5., 2., 4.]),\n",
       "  array([19.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "          0.,  1.,  0.,  0.,  1.,  1.,  4.]),\n",
       "  array([24.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.])],\n",
       " array([  0.  ,  12.75,  25.5 ,  38.25,  51.  ,  63.75,  76.5 ,  89.25,\n",
       "        102.  , 114.75, 127.5 , 140.25, 153.  , 165.75, 178.5 , 191.25,\n",
       "        204.  , 216.75, 229.5 , 242.25, 255.  ]),\n",
       " <a list of 28 Lists of Patches objects>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1f088382c18>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel histogram prior to feature scaling : 6 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([15.,  1.,  2.,  1.,  2.,  2.,  1.,  3.,  0.,  0.,  1.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([ 7.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,\n",
       "          0.,  0., 12.,  4.,  1.,  1.,  0.]),\n",
       "  array([ 6.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "          0.,  6., 12.,  3.,  0.,  0.,  0.]),\n",
       "  array([1., 0., 0., 1., 1., 0., 1., 1., 2., 3., 0., 0., 2., 0., 5., 6., 5.,\n",
       "         0., 0., 0.]),\n",
       "  array([6., 2., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 4., 5., 3.,\n",
       "         1., 1., 0.]),\n",
       "  array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0., 10.,  4.,  3.,  1.,  4.,  5.]),\n",
       "  array([ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          1., 18.,  7.,  0.,  0.,  1.,  0.]),\n",
       "  array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "          1., 14., 11.,  1.,  0.,  0.,  0.]),\n",
       "  array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          2., 22.,  3.,  0.,  1.,  0.,  0.]),\n",
       "  array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "          0., 19.,  7.,  1.,  0.,  0.,  0.]),\n",
       "  array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  2.,\n",
       "          2., 15.,  9.,  0.,  0.,  0.,  0.]),\n",
       "  array([ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  2.,\n",
       "          2., 21.,  2.,  0.,  0.,  0.,  0.]),\n",
       "  array([ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  4.,\n",
       "          0., 19.,  3.,  1.,  0.,  0.,  0.]),\n",
       "  array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,\n",
       "          1., 20.,  4.,  1.,  0.,  0.,  0.]),\n",
       "  array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
       "          1., 21.,  3.,  2.,  0.,  0.,  0.]),\n",
       "  array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
       "          1., 21.,  3.,  1.,  0.,  1.,  0.]),\n",
       "  array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,\n",
       "          1., 20.,  3.,  2.,  0.,  0.,  0.]),\n",
       "  array([ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  2.,\n",
       "          3., 18.,  1.,  1.,  1.,  0.,  1.]),\n",
       "  array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          2., 13.,  2.,  4.,  1.,  5.,  0.]),\n",
       "  array([3., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 2., 8., 6., 2.,\n",
       "         0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 1., 1., 4., 2., 0., 3., 1., 0., 1., 8., 5., 1.,\n",
       "         0., 0., 0.]),\n",
       "  array([ 6.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "          1.,  6., 10.,  1.,  3.,  0.,  0.]),\n",
       "  array([8., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 4., 6., 3.,\n",
       "         5., 0., 0.]),\n",
       "  array([13.,  1.,  2.,  0.,  2.,  1.,  0.,  2.,  1.,  3.,  2.,  0.,  1.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.])],\n",
       " array([  0.  ,  12.75,  25.5 ,  38.25,  51.  ,  63.75,  76.5 ,  89.25,\n",
       "        102.  , 114.75, 127.5 , 140.25, 153.  , 165.75, 178.5 , 191.25,\n",
       "        204.  , 216.75, 229.5 , 242.25, 255.  ]),\n",
       " <a list of 28 Lists of Patches objects>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1f088830cc0>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel histogram prior to feature scaling : 7 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([array([24.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "          1.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([24.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  1.,  0.,  0.,  3.,  0.]),\n",
       "  array([22.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "          1.,  0.,  1.,  1.,  0.,  1.,  0.]),\n",
       "  array([22.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  2.,\n",
       "          0.,  1.,  0.,  0.,  1.,  1.,  0.]),\n",
       "  array([22.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  2.,  0.,\n",
       "          0.,  0.,  0.,  1.,  1.,  1.,  0.]),\n",
       "  array([21.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  2.,  0.,  0.,\n",
       "          0.,  0.,  0.,  1.,  1.,  0.,  1.]),\n",
       "  array([21.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  1.,  1.,  0.,  0.,\n",
       "          0.,  0.,  1.,  0.,  1.,  0.,  1.]),\n",
       "  array([20.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  0.,  1.,  0.,\n",
       "          1.,  0.,  1.,  0.,  1.,  0.,  1.]),\n",
       "  array([20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  2.,  0.,\n",
       "          2.,  0.,  0.,  1.,  1.,  0.,  2.]),\n",
       "  array([19.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,\n",
       "          1.,  0.,  0.,  2.,  1.,  1.,  1.]),\n",
       "  array([19.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,\n",
       "          1.,  0.,  0.,  1.,  2.,  3.,  0.]),\n",
       "  array([18.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  1.,  1.,\n",
       "          0.,  0.,  1.,  2.,  1.,  1.,  1.]),\n",
       "  array([18.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  3.,\n",
       "          0.,  0.,  0.,  2.,  3.,  2.,  0.]),\n",
       "  array([17.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  2.,\n",
       "          0.,  1.,  0.,  1.,  4.,  1.,  0.]),\n",
       "  array([16.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  2.,\n",
       "          1.,  1.,  1.,  2.,  2.,  2.,  0.]),\n",
       "  array([15.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,\n",
       "          0.,  1.,  2.,  2.,  1.,  3.,  1.]),\n",
       "  array([13.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  2.,\n",
       "          0.,  1.,  2.,  2.,  3.,  2.,  1.]),\n",
       "  array([12.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  3.,\n",
       "          1.,  1.,  1.,  1.,  6.,  1.,  1.]),\n",
       "  array([12.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  2.,\n",
       "          0.,  1.,  0.,  2.,  6.,  3.,  0.]),\n",
       "  array([13.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  3.,\n",
       "          1.,  0.,  0.,  1.,  2.,  6.,  0.]),\n",
       "  array([14.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  2.,\n",
       "          0.,  2.,  0.,  2.,  3.,  3.,  0.]),\n",
       "  array([15.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  3.,\n",
       "          1.,  2.,  1.,  0.,  3.,  2.,  0.]),\n",
       "  array([15.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  2.,  3.,\n",
       "          1.,  0.,  1.,  1.,  2.,  0.,  2.]),\n",
       "  array([15.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  2.,  5.,\n",
       "          0.,  1.,  1.,  0.,  1.,  1.,  1.]),\n",
       "  array([15.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  2.,  3.,  2.,\n",
       "          1.,  0.,  2.,  1.,  1.,  1.,  0.]),\n",
       "  array([15.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  2.,  1.,\n",
       "          3.,  1.,  2.,  1.,  2.,  0.,  0.]),\n",
       "  array([14.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  2.,  2.,\n",
       "          3.,  1.,  0.,  0.,  1.,  1.,  2.]),\n",
       "  array([23.,  0.,  1.,  0.,  1.,  3.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.])],\n",
       " array([  0.  ,  12.75,  25.5 ,  38.25,  51.  ,  63.75,  76.5 ,  89.25,\n",
       "        102.  , 114.75, 127.5 , 140.25, 153.  , 165.75, 178.5 , 191.25,\n",
       "        204.  , 216.75, 229.5 , 242.25, 255.  ]),\n",
       " <a list of 28 Lists of Patches objects>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1f0874800b8>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel histogram prior to feature scaling : 8 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([array([25.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([18.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  1.,  6.]),\n",
       "  array([15.,  3.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  2.,  0.,  0.,\n",
       "          1.,  0.,  0.,  1.,  0.,  2.,  2.]),\n",
       "  array([15.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
       "          0.,  1.,  4.,  1.,  0.,  0.,  5.]),\n",
       "  array([5., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 4., 0., 2., 0., 3.,\n",
       "         3., 3., 4.]),\n",
       "  array([5., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 3., 3., 1., 3., 1.,\n",
       "         4., 2., 4.]),\n",
       "  array([5., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 2., 1., 2., 1.,\n",
       "         3., 2., 8.]),\n",
       "  array([ 5.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  2.,\n",
       "          0.,  1.,  0.,  4.,  1.,  2., 10.]),\n",
       "  array([5., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 2., 2., 3.,\n",
       "         2., 2., 7.]),\n",
       "  array([5., 0., 0., 0., 0., 0., 0., 0., 0., 1., 2., 1., 2., 0., 2., 2., 3.,\n",
       "         1., 2., 7.]),\n",
       "  array([5., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 2., 0., 3., 1., 3., 2.,\n",
       "         2., 1., 8.]),\n",
       "  array([5., 0., 0., 0., 0., 0., 1., 0., 0., 0., 2., 1., 2., 0., 2., 3., 1.,\n",
       "         0., 3., 8.]),\n",
       "  array([8., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 4., 0., 1., 3., 0.,\n",
       "         3., 1., 7.]),\n",
       "  array([8., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 3., 3., 0., 3., 2.,\n",
       "         1., 2., 4.]),\n",
       "  array([8., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 3., 1., 0., 1., 2., 0.,\n",
       "         1., 5., 4.]),\n",
       "  array([8., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 3., 1., 1., 0., 2., 1.,\n",
       "         2., 4., 3.]),\n",
       "  array([8., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0., 1., 2., 2., 3., 1., 1.,\n",
       "         1., 4., 3.]),\n",
       "  array([8., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 2., 3., 1., 1.,\n",
       "         2., 4., 4.]),\n",
       "  array([8., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 3., 3., 1., 4.,\n",
       "         1., 2., 4.]),\n",
       "  array([8., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 2., 4., 1., 4.,\n",
       "         1., 2., 4.]),\n",
       "  array([8., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 6., 1., 4.,\n",
       "         1., 3., 3.]),\n",
       "  array([8., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 4., 1., 2., 2.,\n",
       "         2., 5., 2.]),\n",
       "  array([8., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 2., 2., 1., 3.,\n",
       "         4., 3., 3.]),\n",
       "  array([8., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 2., 3., 4., 4.,\n",
       "         4., 0., 1.]),\n",
       "  array([8., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 2., 4., 0.,\n",
       "         9., 1., 0.]),\n",
       "  array([9., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 4., 4., 5.,\n",
       "         3., 1., 0.]),\n",
       "  array([16.,  1.,  0.,  0.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "          0.,  1.,  1.,  1.,  0.,  0.,  5.]),\n",
       "  array([25.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.])],\n",
       " array([  0.  ,  12.75,  25.5 ,  38.25,  51.  ,  63.75,  76.5 ,  89.25,\n",
       "        102.  , 114.75, 127.5 , 140.25, 153.  , 165.75, 178.5 , 191.25,\n",
       "        204.  , 216.75, 229.5 , 242.25, 255.  ]),\n",
       " <a list of 28 Lists of Patches objects>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1f087103400>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel histogram prior to feature scaling : 9 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([array([26.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([24.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  2.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  1.,  0.,  0.]),\n",
       "  array([24.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,\n",
       "          0.,  2.,  0.,  0.,  0.,  0.,  0.]),\n",
       "  array([23.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,\n",
       "          1.,  0.,  0.,  0.,  1.,  0.,  0.]),\n",
       "  array([22.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  1.,  0.,\n",
       "          1.,  0.,  0.,  0.,  1.,  0.,  0.]),\n",
       "  array([22.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  1.,  1.,\n",
       "          1.,  0.,  0.,  1.,  0.,  0.,  0.]),\n",
       "  array([22.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  2.,  0.,  1.,\n",
       "          0.,  0.,  0.,  2.,  0.,  0.,  0.]),\n",
       "  array([21.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  3.,  0.,\n",
       "          0.,  1.,  0.,  0.,  0.,  1.,  0.]),\n",
       "  array([21.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  2.,  1.,  0.,\n",
       "          1.,  1.,  0.,  0.,  0.,  0.,  1.]),\n",
       "  array([20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  2.,  1.,  0.,  1.,\n",
       "          3.,  0.,  0.,  1.,  0.,  0.,  0.]),\n",
       "  array([18.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  2.,  1.,  1.,\n",
       "          4.,  0.,  0.,  1.,  0.,  0.,  0.]),\n",
       "  array([17.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  2.,  2.,\n",
       "          2.,  2.,  0.,  0.,  0.,  0.,  1.]),\n",
       "  array([15.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  3.,  2.,\n",
       "          3.,  1.,  1.,  1.,  0.,  0.,  0.]),\n",
       "  array([10.,  0.,  1.,  1.,  1.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  8.,\n",
       "          2.,  2.,  0.,  0.,  1.,  0.,  0.]),\n",
       "  array([11.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  2.,  5.,\n",
       "          2.,  4.,  2.,  0.,  1.,  0.,  0.]),\n",
       "  array([11.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  1.,  5.,\n",
       "          6.,  0.,  2.,  1.,  0.,  0.,  0.]),\n",
       "  array([12.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  2.,  5.,\n",
       "          4.,  3.,  0.,  1.,  0.,  0.,  0.]),\n",
       "  array([12.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  2.,\n",
       "          6.,  4.,  0.,  0.,  0.,  0.,  1.]),\n",
       "  array([13.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  4.,\n",
       "          6.,  3.,  1.,  1.,  0.,  0.,  0.]),\n",
       "  array([11.,  2.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  3.,\n",
       "          4.,  6.,  0.,  1.,  0.,  0.,  0.]),\n",
       "  array([10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  2.,\n",
       "          5.,  6.,  2.,  0.,  1.,  1.,  1.]),\n",
       "  array([10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "          5., 11.,  1.,  0.,  0.,  0.,  0.]),\n",
       "  array([10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  2.,\n",
       "          6.,  9.,  0.,  0.,  1.,  0.,  0.]),\n",
       "  array([10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  3.,\n",
       "         10.,  4.,  0.,  0.,  1.,  0.,  0.]),\n",
       "  array([10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  3.,\n",
       "          8.,  5.,  0.,  0.,  2.,  0.,  0.]),\n",
       "  array([10.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  2.,  0.,  0.,  1.,  3.,\n",
       "          3.,  7.,  1.,  0.,  0.,  0.,  0.]),\n",
       "  array([15.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "          2.,  6.,  3.,  0.,  0.,  0.,  0.]),\n",
       "  array([20.,  4.,  1.,  1.,  0.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.])],\n",
       " array([  0.  ,  12.75,  25.5 ,  38.25,  51.  ,  63.75,  76.5 ,  89.25,\n",
       "        102.  , 114.75, 127.5 , 140.25, 153.  , 165.75, 178.5 , 191.25,\n",
       "        204.  , 216.75, 229.5 , 242.25, 255.  ]),\n",
       " <a list of 28 Lists of Patches objects>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1f087110128>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.tight_layout>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIEAAAYiCAYAAACrIfyoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f085d071d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_class_data = np.vectorize(lambda arr: label==i )\n",
    "\n",
    "fig, a = plt.subplots(5,2, sharex='row', sharey='col')\n",
    "fig.set_size_inches(20,25)\n",
    "fig.subplots_adjust(hspace = 0.3, wspace = 0.1)\n",
    "fig.suptitle('Pixel value - Histogram and distribution', fontsize = 16)\n",
    "\n",
    "classes = np.unique(label)\n",
    "a = a.ravel()\n",
    "        \n",
    "for i,ax in enumerate(a):\n",
    "    print(\"Pixel histogram prior to feature scaling : {} \".format(classes[i],i))\n",
    "    #print(i)\n",
    "\n",
    "    #ax=[]\n",
    "    d = data[get_class_data(i)]\n",
    "    ax.hist(d[i], bins = 20)\n",
    "    ax.set_title(i, fontsize = 16)    \n",
    "        \n",
    "    \n",
    "plt.tight_layout    \n",
    "plt.savefig(\"PixelHist_Distiribution.png\", dpi = 300)\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For reference: https://github.com/zalandoresearch/fashion-mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "| Label | Description   |\n",
    "|------|------|\n",
    "|   0  | T-shirt/top|\n",
    "|   1  | Trouser|\n",
    "|   2  | Pullover|\n",
    "|   3  | Dress|\n",
    "|   4  | Coat|\n",
    "|   5  | Sandal|\n",
    "|   6  | Shirt|\n",
    "|   7  | Sneaker|\n",
    "|   8  | Bag|\n",
    "|   9  | Ankle boot|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "############IGNORE########################################################\n",
    "############IGNORE########################################################\n",
    "############IGNORE########################################################\n",
    "\n",
    "data, label, data_test, label_test = read_data_in(desktop_or_laptop='d')\n",
    "\n",
    "train_data = data[:30000]\n",
    "\n",
    "X_train = train_data.reshape(-1, 784)\n",
    "\n",
    "\n",
    "#def is_symmetric(X, tolerance = 1e-9):\n",
    "#    return(np.allclose(X,X.T, atol=tolerance))\n",
    "\n",
    "\n",
    " #= data.reshape[-1,784]\n",
    "#is_sparse(X_train,X_train.shape[0],X_train.shape[1])\n",
    "#X.shape\n",
    "#scpy.sparse.isspmatrix(train_data)\n",
    "\n",
    "#X_train.shape\n",
    "#data.reshape[-1,784]\n",
    "#data[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############IGNORE########################################################\n",
    "############IGNORE########################################################\n",
    "############IGNORE########################################################\n",
    "i = 10\n",
    "#show_image(data[i],label[i])\n",
    "\n",
    "#print('Total training data shape : ', data.shape, label.shape)\n",
    "#data_train = data_train / 255\n",
    "\n",
    "\n",
    "_, axarr = plt.subplots(10,10,figsize=(10,10))\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "       data[np.random.randint(data.shape[0])] \n",
    "       axarr[i,j].axis('off')     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to train and preduct using LogisticRegression - One vs Rest!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\New\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:180: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of records in X train: 29700\n",
      "Total number of records in y train: 29700\n",
      "Total number of features in X train: 784\n",
      "Total number of records in X train (valdation set): 300\n",
      "Total number of features in X train (validation set): 784\n",
      "Total number of records in X test: 5000\n",
      "Total number of features in X test: 784\n",
      "#################################################\n",
      "Logistic Regression model initialised with Max iterations = 500, K (classes) = 10, lmbda (Regularisation parameter) =1\n",
      "#################################################\n",
      "                                            \n",
      "############################################\n",
      "### Gradient Descent Optimisation beginning\n",
      "############################################\n",
      "Class 0 being optimised\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.101262\n",
      "         Iterations: 30\n",
      "         Function evaluations: 65\n",
      "         Gradient evaluations: 65\n",
      "Class 1 being optimised\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.021304\n",
      "         Iterations: 20\n",
      "         Function evaluations: 45\n",
      "         Gradient evaluations: 45\n",
      "Class 2 being optimised\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.132361\n",
      "         Iterations: 60\n",
      "         Function evaluations: 124\n",
      "         Gradient evaluations: 124\n",
      "Class 3 being optimised\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.083403\n",
      "         Iterations: 25\n",
      "         Function evaluations: 55\n",
      "         Gradient evaluations: 55\n",
      "Class 4 being optimised\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.119090\n",
      "         Iterations: 56\n",
      "         Function evaluations: 122\n",
      "         Gradient evaluations: 122\n",
      "Class 5 being optimised\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.046474\n",
      "         Iterations: 29\n",
      "         Function evaluations: 59\n",
      "         Gradient evaluations: 59\n",
      "Class 6 being optimised\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.173625\n",
      "         Iterations: 48\n",
      "         Function evaluations: 95\n",
      "         Gradient evaluations: 95\n",
      "Class 7 being optimised\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.054991\n",
      "         Iterations: 22\n",
      "         Function evaluations: 43\n",
      "         Gradient evaluations: 43\n",
      "Class 8 being optimised\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.044944\n",
      "         Iterations: 29\n",
      "         Function evaluations: 63\n",
      "         Gradient evaluations: 63\n",
      "Class 9 being optimised\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.034726\n",
      "         Iterations: 40\n",
      "         Function evaluations: 77\n",
      "         Gradient evaluations: 77\n",
      "###########################################\n",
      "### Gradient Descent Optimisation finished\n",
      "###########################################\n",
      "Probability averages on train: ,85.88552188552188\n",
      "Probability averages on test: ,84.25\n",
      "Wall time: 24.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Hyper-parameter tuning - uncomment the first value if you want to loop\n",
    "#lambda_values = [0.1, 0.5, 1, 1.5, 2, 2.5, 3]\n",
    "lambda_values = [1]\n",
    "\n",
    "prediction_average_list = []\n",
    "prediction_average_test_list = []    \n",
    "\n",
    "for i in lambda_values:\n",
    "\n",
    "    # Read the source files in\n",
    "    # Ben/Stef - You'll need to change this\n",
    "    data, label, data_test, label_test = read_data_in(desktop_or_laptop='d')\n",
    "\n",
    "    # Declare variables for instantiation of the LogisticRegression Class\n",
    "    #lmbda = 2.5\n",
    "    k = 10\n",
    "    intercept = True\n",
    "    max_iter = 500\n",
    "\n",
    "    # THIS IS A HACK, NOT SURE WHY TEST DATA ISN'T BEING READ IN PROPERLY CURRENTLY\n",
    "    data_test = data_test[:2000]\n",
    "\n",
    "    #########################################################################################\n",
    "    # Once data is read in from file data, we split it for training, validation and testing #\n",
    "    # standardise, & re-shape it process                                                    #\n",
    "    #########################################################################################\n",
    "        \n",
    "    X_train,X_validate,X_test,y_train,y_validate,y_test = shape_matrix_process(split_percent=0.99)\n",
    "    X_test = X_test[:2000]\n",
    "\n",
    "\n",
    "    ############################################\n",
    "    # Instantiate our logistic Regression model#\n",
    "    ############################################\n",
    "    print('#################################################')\n",
    "    model = LogisticRegression(max_iter, intercept, k, i)\n",
    "    print('#################################################')\n",
    "\n",
    "    # Carry out principal component analysis and project back with 99% of variance\n",
    "    #X_train, X_reduced  = compress_project(percent=.99,data_to_compress=X_train)\n",
    "\n",
    "    # Add the intercept value\n",
    "    X_train,n = model.add_intercept(X_train,y_train)\n",
    "\n",
    "    # Create a theta matrix to capture values of theta when we minimis the objective function below\n",
    "    theta = np.zeros((k,n)) #inital parameters\n",
    "\n",
    "    #############################################################################################\n",
    "    # Using conjugate gradient, we attempt to carry out optimisation to find theta and therefore#\n",
    "    # Using theta and the data features to predict each class   #################################\n",
    "    #############################################################################################\n",
    "    \n",
    "    # Do you want to see a verbose/full output for conjugate gradient descent?\n",
    "    \n",
    "    #theta = model.fit(X_mat=X_train,max_iter=500,y_mat=y_train,n=n, full_outp=False)\n",
    "    \n",
    "    \n",
    "    #theta, fopt, func_calls, grad_calls,warnflag  = model.fit(X_mat=X_train,max_iter=500,y_mat=y_train,n=n, full_outp=False)\n",
    "    theta  = model.fit(X_mat=X_train,max_iter=500,y_mat=y_train,n=n, full_outp=False)\n",
    "            \n",
    "    #######################################################################################\n",
    "    # Carry out the predictions based on the trained model and then carry out predictions #\n",
    "    # on the unseen test data                                                             #\n",
    "    #######################################################################################\n",
    "    X_test,n = model.add_intercept(X_test,y_test)\n",
    "\n",
    "    \n",
    "    preds, prediction_average = model.predict(X=X_train,y=y_train,theta=theta)\n",
    "    prediction_average_list.append(prediction_average)\n",
    "    \n",
    "    \n",
    "    \n",
    "    preds_test,prediction_average_test = model.predict(X_test,y_test,theta=theta)    \n",
    "    prediction_average_test_list.append(prediction_average_test)\n",
    "    \n",
    "print(\"Probability averages on train: \",*prediction_average_list, sep =',')\n",
    "print(\"Probability averages on test: \",*prediction_average_test_list, sep =',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Explicitly getting prediction results\n",
    "\n",
    "pred_train = []\n",
    "pred_train = np.argmax(X_train @ theta.T, axis = 1)\n",
    "pred = [e if e else 0 for e in pred_train]\n",
    "np.mean(pred_train == y_train.flatten()) * 100\n",
    "\n",
    "pred_test = []\n",
    "pred_test = np.argmax(X_test @ theta.T, axis = 1)\n",
    "pred = [e if e else 0 for e in pred_test]\n",
    "np.mean(pred_test == y_test.flatten()) * 100\n",
    "\n",
    "#79.74166666666666"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(pred_test)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test, preds_test)\n",
    "#cm = metrics.confusion_matrix(y_train, preds)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, preds_test))\n",
    "\n",
    "#print(classification_report(y_train, preds_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#cm = compute_confusion_matrix(y_test, preds_test)\n",
    "\n",
    "cm = compute_confusion_matrix(y_train, preds)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(8,8)\n",
    "\n",
    "im = ax.imshow(cm, cmap='Oranges')\n",
    "\n",
    "# Take the confusion matrix array, and make it pretty with colours and text :)\n",
    "\n",
    "# A loop to go through every class and plot value for each, with respective colour on the Yellow Green matplotlib colour scale\n",
    "for i in range(len(np.unique(y_test))):\n",
    "    for j in range(len(np.unique(y_test))):\n",
    "        text = ax.text(j, i, cm[i, j],\n",
    "                       ha=\"center\", va=\"center\", color=\"k\")\n",
    "plt.title(\"Confusion matrix - Train Dataset\", fontsize = 16)        \n",
    "#plt.title(\"Confusion matrix - Test Dataset\", fontsize = 16)        \n",
    "\n",
    "plt.savefig(\"ConfusionMatrix_Train.png\", dpi = 300)\n",
    "#plt.savefig(\"ConfusionMatrix_Test.png\", dpi = 300)\n",
    "plt.xlabel(\"Predicted classes \", fontsize = 16)   \n",
    "plt.ylabel(\"Actual classes \", fontsize = 16)   \n",
    "fig.tight_layout()        \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr[2], tpr[2], color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
