{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Utility functions and general imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import scipy as scpy\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from scipy import optimize as opt\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Creating the functions used throughout the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining utility functions for reading images and data in\n",
    "def show_image(array, label):\n",
    "    im = Image.fromarray(array)\n",
    "    print(\"Item of label={}\".format(label))\n",
    "    return(imshow(im))\n",
    "\n",
    "def precision(label, confusion_matrix):\n",
    "    col = confusion_matrix[:, label]\n",
    "    return confusion_matrix[label, label] / col.sum()\n",
    "\n",
    "def precision_average(confusion_matrix):\n",
    "    rows, columns = confusion_matrix.shape\n",
    "    sum_of_precisions = 0\n",
    "    for label in range(rows):\n",
    "        sum_of_precisions += precision(label, confusion_matrix)\n",
    "    return sum_of_precisions / rows\n",
    "\n",
    "def recall(label, confusion_matrix):\n",
    "    row = confusion_matrix[label, :]\n",
    "    return confusion_matrix[label, label] / row.sum()    \n",
    "\n",
    "def recall_average(confusion_matrix):\n",
    "    rows, columns = confusion_matrix.shape\n",
    "    sum_of_recalls = 0\n",
    "    for label in range(columns):\n",
    "        sum_of_recalls += recall(label, confusion_matrix)\n",
    "    return sum_of_recalls / columns\n",
    "\n",
    "\n",
    "def calculate_acc_metrics(cm):\n",
    "    tp = []\n",
    "    tn = []\n",
    "    fp = []\n",
    "    fn = []\n",
    "        \n",
    "    for i in range(0):\n",
    "\n",
    "\n",
    "        tp.append(cm[i][i])\n",
    "        fp.append(sum(cm[:,i]) - cm[i][i])\n",
    "        fn.append(sum(cm[i,:]) - cm[i][i])\n",
    "        tn.append(np.sum(cm) - (tp + fp + fn))\n",
    "\n",
    "    return(tp)\n",
    "\n",
    "def create_confusion_matrix(actual, preds):\n",
    "    \n",
    "  classes = len(np.unique(actual)) # Number of classes \n",
    "  \n",
    "  cm = np.zeros((classes, classes))\n",
    "\n",
    "  for i in range(len(actual)):\n",
    "    cm[actual[i]][preds[i]] += 1\n",
    "    \n",
    "  return(cm)   \n",
    "\n",
    "def shape_matrix_process(split_percent):\n",
    "    \n",
    "            \n",
    "        # Read the source files in from file\n",
    "        data, label, test_data, test_label = read_data_in()\n",
    "        \n",
    "        # Creating a % split of training and validation data for the \n",
    "        # training model to be trained on\n",
    "        indices = range(data.shape[0])\n",
    "        classes = np.unique(label)\n",
    "        \n",
    "        training_records = int(split_percent * data.shape[0])        \n",
    "        \n",
    "        # Get the records that are part of the indices declared above\n",
    "        train_data = data[:training_records]\n",
    "        validate_data = data[training_records:]        \n",
    "        \n",
    "        # Get the labels that are part of the indices declared above\n",
    "        label_train = label[:training_records]\n",
    "        label_validate = label[training_records:]        \n",
    "        \n",
    "        \n",
    "        # We can vectorise our array lambda checker to build out the pixel distributions\n",
    "        #get_class_data = np.vectorize(lambda arr: label==i )\n",
    "                        \n",
    "        #for i in classes:\n",
    "        #    print(\"Pixel histogram prior to feature scaling : \".format(classes[i]))\n",
    "        #    print(i)\n",
    "        # \n",
    "        #    d=[]\n",
    "        #    d = data[get_class_data(i)]\n",
    "        #    h = plt.hist(d[i].ravel())\n",
    "        #    plt.show()\n",
    "\n",
    "        X_train = train_data.reshape(-1, 784)\n",
    "        X_validate = validate_data.reshape(-1, 784)        \n",
    "                \n",
    "        # Change our X_train data set to ensure later normalisation technique doesn't raise warnings on the data type\n",
    "        X_train = X_train.astype('float32')\n",
    "        # Change our X_Validate data set to ensure later normalisation technique doesn't raise warnings on the data type\n",
    "        X_validate = X_validate.astype('float32')\n",
    "        \n",
    "        y_train = np.ravel(label_train)\n",
    "        y_validate = np.ravel(label_validate)                    \n",
    "\n",
    "               \n",
    "        ########################\n",
    "        # Now to transform test#\n",
    "        ########################                  \n",
    "        X_test = test_data.reshape(-1,784)\n",
    "        y_test = test_label [:2000]       \n",
    "        \n",
    "        X_test = X_test.astype('float32')\n",
    "                    \n",
    "        ##########################################################################  \n",
    "        # MINMAXSCALER IS THE ONLY SCALER FOUND TO RELIABLY RESULT IN CONVERGENCE*\n",
    "        ##########################################################################\n",
    "                    \n",
    "        # Normalise our training, validation & test-sets\n",
    "        X_train = normalise(X_train)                                \n",
    "        X_validate = normalise(X_validate)\n",
    "        X_test = normalise(X_test)                \n",
    "        \n",
    "        print('Total number of records in X train: {}'.format(X_train.shape[0]))\n",
    "        print('Total number of records in y train: {}'.format(y_train.shape[0]))\n",
    "        print('Total number of features in X train: {}'.format(X_train.shape[1]))\n",
    "\n",
    "        print('Total number of records in X train (valdation set): {}'.format(X_validate.shape[0]))\n",
    "        print('Total number of features in X train (validation set): {}'.format(X_validate.shape[1]))\n",
    "                \n",
    "\n",
    "        ##########################################################################  \n",
    "        # MINMAXSCALER IS THE ONLY SCALER FOUND TO RELIABLY RESULT IN CONVERGENCE*\n",
    "        # HOWEVER NEEDS FURTHER TESTING =========================================#\n",
    "        ##########################################################################\n",
    "   \n",
    "\n",
    "                                          \n",
    "        print('Total number of records in X test: {}'.format(X_test.shape[0]))\n",
    "        print('Total number of features in X test: {}'.format(X_test.shape[1]))\n",
    "        \n",
    "        #Return our training, validation & test sets\n",
    "        return(X_train,X_validate,X_test,y_train,y_validate,y_test)\n",
    "    \n",
    "    \n",
    "# Import the files in from python h5 format\n",
    "def read_data_in():\n",
    "\n",
    "    ## Dan's desktop folder location - NEEDS CHANGING\n",
    "    with h5py.File('../Input//images_training.h5','r') as H:\n",
    "        data = np.copy(H['data'])\n",
    "    with h5py.File('../Input/labels_training.h5','r') as H:\n",
    "        label = np.copy(H['label'])\n",
    "    with h5py.File('../Input/images_testing.h5','r') as H:\n",
    "        data_test = np.copy(H['data'])\n",
    "    with h5py.File('../Input/labels_testing_2000.h5','r') as H:\n",
    "        label_test = np.copy(H['label'])     \n",
    "    \n",
    "    return(data, label, data_test, label_test)\n",
    "\n",
    "# Is the matrix symmetric?\n",
    "def is_symmetric(X, tolerance = 1e-9):\n",
    "    return(np.allclose(X,X.T, atol=tolerance))\n",
    "\n",
    "def normalise(X):    \n",
    "    X_scaled = (X - np.min(X,axis=0)) / (np.max(X, axis=0) - (np.min(X,axis=0) ))\n",
    "    return(X_scaled)\n",
    "\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self,  max_iter, intercept, k, lmbda ):\n",
    "        #self.lr = lr\n",
    "        self.max_iter = max_iter\n",
    "        self.intercept = intercept\n",
    "        self.k = k\n",
    "        self.lmbda = lmbda\n",
    "        print (\"Logistic Regression model initialised with Max iterations = {}, K (classes) = {}, lmbda (Regularisation parameter) ={}\".format(self.max_iter,self.k, self.lmbda))\n",
    "    \n",
    "    def add_intercept(self, X_mat,y_mat):\n",
    "        m = len(y_mat)\n",
    "        ones = np.ones((m,1))\n",
    "\n",
    "        X_mat = np.concatenate((ones,X_mat),axis=1)\n",
    "        m_shape,n_shape = X_mat.shape\n",
    "        \n",
    "        return(X_mat,n_shape)\n",
    "    \n",
    "    # Defining the sigmoid function required in LR\n",
    "    def __sigmoid(self, z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    \n",
    "    def fit(self, X_mat,max_iter, y_mat,n, full_outp):\n",
    "                        \n",
    "        theta = np.zeros((k,n)) #inital parameters                        \n",
    "        print(\"                                            \")\n",
    "        print(\"############################################\")\n",
    "        print(\"### Gradient Descent Optimisation beginning\")\n",
    "        print(\"############################################\")\n",
    "        for i in range(k):\n",
    "            label_class = i if i else 0\n",
    "            print('Class {} being optimised'.format(i))\n",
    "            theta[i] = opt.fmin_cg(f = self.__cost_func_vectorised, x0 = theta[i].flatten(), gtol = 1e-03, fprime = self.__gradient_reg_vectorised, args = (X_mat,(y_mat == label_class).flatten(), self.lmbda),maxiter= max_iter, disp = True, full_output = full_outp)                                    \n",
    "        print(\"###########################################\")\n",
    "        print(\"### Gradient Descent Optimisation finished\")\n",
    "        print(\"###########################################\")\n",
    "\n",
    "        return(theta)      \n",
    "    \n",
    "    # Vectorised gradient\n",
    "    def __gradient_reg_vectorised(self, theta, X_arr, y_arr, lmbda):\n",
    "        m = len(y_arr)\n",
    "        temp = self.__sigmoid(np.dot(X_arr, theta)) - y_arr\n",
    "        temp = np.dot(temp.T, X_arr).T / m + theta * lmbda / m\n",
    "        temp[0] = temp[0] - theta[0] * lmbda / m\n",
    "        return temp\n",
    "    \n",
    "    # Vectorised cost function\n",
    "    def __cost_func_vectorised(self,theta, X_arr, y_arr, lmbda):\n",
    "        m = len(y_arr)\n",
    "        temp_weight1 = np.multiply(y_arr, np.log(self.__sigmoid(np.dot(X_arr, theta))))\n",
    "        temp_weight2 = np.multiply(1-y_arr, np.log(1-self.__sigmoid(np.dot(X_arr, theta))))\n",
    "        \n",
    "        # Printing out cost function value\n",
    "        #print(np.sum(temp_weight1 + temp_weight2) / (-m) + np.sum(theta[1:]**2) * self.lmbda / (2*m))\n",
    "        return np.sum(temp_weight1 + temp_weight2) / (-m) + np.sum(theta[1:]**2) * self.lmbda / (2*m)    \n",
    "    \n",
    "    def predict(self,X,y,theta):    \n",
    "        preds = []\n",
    "        preds = np.argmax(X @ theta.T, axis = 1)\n",
    "        #preds = X @ theta.T\n",
    "        preds = [e if e else 0 for e in preds]\n",
    "        average_pred = np.mean(preds == y.flatten()) * 100        \n",
    "        return(preds, average_pred)        \n",
    "        \n",
    "        \n",
    "    def final_predict(self,X,theta):    \n",
    "        preds = []\n",
    "        preds = np.argmax(X @ theta.T, axis = 1)        \n",
    "        preds = [e if e else 0 for e in preds]     \n",
    "        preds = np.asarray(preds)\n",
    "        return(preds)\n",
    "    \n",
    "    def predict_prob(self,X, theta):\n",
    "        return (self.__sigmoid(np.dot(X),theta.T))\n",
    "    \n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'label' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7e172653652c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# How many unique classes are we dealing with? Do we need to perform any sampling for class imbalance?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_nclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'label' is not defined"
     ]
    }
   ],
   "source": [
    "# How many unique classes are we dealing with? Do we need to perform any sampling for class imbalance?\n",
    "classes = np.unique(label)\n",
    "nclasses = len(classes)\n",
    "test_classes = np.unique(label_test)\n",
    "test_nclasses = len(test_classes)\n",
    "\n",
    "print('Total number of classes in Train: ', nclasses)\n",
    "print('Total number of classes in Test : ', test_nclasses)\n",
    "print('Classes to classify in Train are : ', classes)\n",
    "print('Classes to classify in Test are : ', test_classes)\n",
    "\n",
    "\n",
    "unique, counts = np.unique(label, return_counts=True)\n",
    "print('Distribution of labels against total population in Train:')\n",
    "dict(zip(unique,counts))\n",
    "\n",
    "unique_test, count_test = np.unique(label_test, return_counts=True)\n",
    "print('Distribution of labels against total population in Test:')\n",
    "dict(zip(unique_test,count_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pixel histogram prior to feature scaling of test data : \".format(classes[i]))\n",
    "print(i)        \n",
    "d=[]\n",
    "d = data_test[get_class_data(1)]\n",
    "#h = plt.hist(d[i].ravel())\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_class_data = np.vectorize(lambda arr: label==i )\n",
    "\n",
    "fig, a = plt.subplots(5,2, sharex='row', sharey='col')\n",
    "fig.set_size_inches(20,25)\n",
    "fig.subplots_adjust(hspace = 0.3, wspace = 0.1)\n",
    "fig.suptitle('Pixel value - Histogram and distribution', fontsize = 16)\n",
    "\n",
    "classes = np.unique(label)\n",
    "a = a.ravel()\n",
    "        \n",
    "for i,ax in enumerate(a):\n",
    "    print(\"Pixel histogram prior to feature scaling : {} \".format(classes[i],i))\n",
    "    #print(i)\n",
    "\n",
    "    #ax=[]\n",
    "    d = data[get_class_data(i)]\n",
    "    ax.hist(d[i], bins = 20)\n",
    "    ax.set_title(i, fontsize = 16)    \n",
    "        \n",
    "    \n",
    "plt.tight_layout    \n",
    "plt.savefig(\"PixelHist_Distiribution.png\", dpi = 300)\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For reference: https://github.com/zalandoresearch/fashion-mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "| Label | Description   |\n",
    "|------|------|\n",
    "|   0  | T-shirt/top|\n",
    "|   1  | Trouser|\n",
    "|   2  | Pullover|\n",
    "|   3  | Dress|\n",
    "|   4  | Coat|\n",
    "|   5  | Sandal|\n",
    "|   6  | Shirt|\n",
    "|   7  | Sneaker|\n",
    "|   8  | Bag|\n",
    "|   9  | Ankle boot|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to train and predct using LogisticRegression - One vs Rest!\n",
    "# Hyper-parameter tuning stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of records in X train: 24000\n",
      "Total number of records in y train: 24000\n",
      "Total number of features in X train: 784\n",
      "Total number of records in X train (valdation set): 6000\n",
      "Total number of features in X train (validation set): 784\n",
      "Total number of records in X test: 5000\n",
      "Total number of features in X test: 784\n",
      "#################################################\n",
      "Logistic Regression model initialised with Max iterations = 500, K (classes) = 10, lmbda (Regularisation parameter) =1\n",
      "#################################################\n",
      "                                            \n",
      "############################################\n",
      "### Gradient Descent Optimisation beginning\n",
      "############################################\n",
      "Class 0 being optimised\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.102454\n",
      "         Iterations: 26\n",
      "         Function evaluations: 52\n",
      "         Gradient evaluations: 52\n",
      "Class 1 being optimised\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.024978\n",
      "         Iterations: 19\n",
      "         Function evaluations: 39\n",
      "         Gradient evaluations: 39\n",
      "Class 2 being optimised\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.135070\n",
      "         Iterations: 45\n",
      "         Function evaluations: 94\n",
      "         Gradient evaluations: 94\n",
      "Class 3 being optimised\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.084982\n",
      "         Iterations: 29\n",
      "         Function evaluations: 64\n",
      "         Gradient evaluations: 64\n",
      "Class 4 being optimised\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.122483\n",
      "         Iterations: 50\n",
      "         Function evaluations: 97\n",
      "         Gradient evaluations: 97\n",
      "Class 5 being optimised\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.047569\n",
      "         Iterations: 27\n",
      "         Function evaluations: 51\n",
      "         Gradient evaluations: 51\n",
      "Class 6 being optimised\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.175171\n",
      "         Iterations: 40\n",
      "         Function evaluations: 81\n",
      "         Gradient evaluations: 81\n",
      "Class 7 being optimised\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.055070\n",
      "         Iterations: 16\n",
      "         Function evaluations: 33\n",
      "         Gradient evaluations: 33\n",
      "Class 8 being optimised\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.044421\n",
      "         Iterations: 32\n",
      "         Function evaluations: 64\n",
      "         Gradient evaluations: 64\n",
      "Class 9 being optimised\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.036894\n",
      "         Iterations: 32\n",
      "         Function evaluations: 60\n",
      "         Gradient evaluations: 60\n",
      "###########################################\n",
      "### Gradient Descent Optimisation finished\n",
      "###########################################\n",
      "Probability averages on train: ,85.36666666666667\n",
      "Probability averages on validation: ,84.95\n",
      "CPU times: user 31.8 s, sys: 445 ms, total: 32.2 s\n",
      "Wall time: 17.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Hyper-parameter tuning - uncomment the first value if you want to loop\n",
    "#lambda_values = [0.1, 0.5, 1, 2.5, 5]\n",
    "lambda_values = [1]\n",
    "#split_percent_values []\n",
    "\n",
    "prediction_average_list = []\n",
    "prediction_average_validate_list = []\n",
    "prediction_average_test_list = []    \n",
    "\n",
    "for i in lambda_values:\n",
    "\n",
    "    # Read the source files in\n",
    "    # Ben/Stef - You'll need to change this\n",
    "    data, label, data_test, label_test = read_data_in()\n",
    "\n",
    "    # Declare variables for instantiation of the LogisticRegression Class\n",
    "    #lmbda = 2.5\n",
    "    k = 10\n",
    "    intercept = True\n",
    "    max_iter = 500\n",
    "    \n",
    "    data_test = data_test[:2000]\n",
    "\n",
    "    #########################################################################################\n",
    "    # Once data is read in from file data, we split it for training, validation and testing #\n",
    "    # standardise, & re-shape it process                                                    #\n",
    "    #########################################################################################            \n",
    "    X_train,X_validate,X_test,y_train,y_validate,y_test = shape_matrix_process(split_percent=0.99)    \n",
    "    X_test = X_test[:2000]\n",
    "\n",
    "    ############################################\n",
    "    # Instantiate our logistic Regression model#\n",
    "    ############################################\n",
    "    print('#################################################')\n",
    "    model = LogisticRegression(max_iter, intercept, k, i)\n",
    "    print('#################################################')\n",
    "\n",
    "    # Carry out principal component analysis and project back with 99% of variance\n",
    "    #X_train, X_reduced  = compress_project(percent=.99,data_to_compress=X_train)\n",
    "\n",
    "    # Add the intercept value\n",
    "    X_train,n = model.add_intercept(X_train,y_train)\n",
    "    X_validate,n = model.add_intercept(X_validate,y_validate)\n",
    "\n",
    "    # Create a theta matrix to capture values of theta when we minimis the objective function below\n",
    "    theta = np.zeros((k,n)) #inital parameters\n",
    "\n",
    "    #############################################################################################\n",
    "    # Using conjugate gradient, we attempt to carry out optimisation to find theta and therefore#\n",
    "    # Using theta and the data features to predict each class   #################################\n",
    "    #############################################################################################\n",
    "    \n",
    "    # Do you want to see a verbose/full output for conjugate gradient descent?\n",
    "    \n",
    "    #theta = model.fit(X_mat=X_train,max_iter=500,y_mat=y_train,n=n, full_outp=False)\n",
    "    \n",
    "    \n",
    "    #theta, fopt, func_calls, grad_calls,warnflag  = model.fit(X_mat=X_train,max_iter=500,y_mat=y_train,n=n, full_outp=False)\n",
    "    theta  = model.fit(X_mat=X_train,max_iter=2000,y_mat=y_train,n=n, full_outp=False)\n",
    "            \n",
    "    #######################################################################################\n",
    "    # Carry out the predictions based on the trained model and then carry out predictions #\n",
    "    # on the unseen test data                                                             #\n",
    "    #######################################################################################\n",
    "    X_test,n = model.add_intercept(X_test,y_test)\n",
    "    \n",
    "    preds, prediction_average = model.predict(X=X_train,y=y_train,theta=theta)    \n",
    "    prediction_average_list.append(prediction_average)\n",
    "        \n",
    "    preds_validate, prediction_average_validate = model.predict(X=X_validate,y=y_validate,theta=theta)\n",
    "    prediction_average_validate_list.append(prediction_average_validate)\n",
    "            \n",
    "    preds_test,prediction_average_test = model.predict(X_test,y_test,theta=theta)    \n",
    "    prediction_average_test_list.append(prediction_average_test)\n",
    "    \n",
    "    cm_train = create_confusion_matrix(y_train, preds)\n",
    "    cm = create_confusion_matrix(y_validate, preds_validate)\n",
    "    \n",
    "    print(\"Probability averages on train: \",*prediction_average_list, sep =',')\n",
    "    print(\"Probability averages on validation: \",*prediction_average_validate_list, sep =',')    \n",
    "    #print(\"precision total - train: \", precision_average(cm_train))    \n",
    "    #print(\"precision total - validation: \", precision_average(cm))\n",
    "    #print(\"recall total - train: \", recall_average(cm_train))   \n",
    "    #print(\"recall total - validation: \", recall_average(cm))   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final model execution - Based on hyper-parameter tuning from above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danbridgman/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:162: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/danbridgman/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:162: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of records in X train: 29700\n",
      "Total number of records in y train: 29700\n",
      "Total number of features in X train: 784\n",
      "Total number of records in X train (valdation set): 300\n",
      "Total number of features in X train (validation set): 784\n",
      "Total number of records in X test: 5000\n",
      "Total number of features in X test: 784\n",
      "#################################################\n",
      "Logistic Regression model initialised with Max iterations = 500, K (classes) = 10, lmbda (Regularisation parameter) =1\n",
      "#################################################\n",
      "                                            \n",
      "############################################\n",
      "### Gradient Descent Optimisation beginning\n",
      "############################################\n",
      "Class 0 being optimised\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.101262\n",
      "         Iterations: 30\n",
      "         Function evaluations: 65\n",
      "         Gradient evaluations: 65\n",
      "Class 1 being optimised\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.021304\n",
      "         Iterations: 20\n",
      "         Function evaluations: 45\n",
      "         Gradient evaluations: 45\n",
      "Class 2 being optimised\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.133686\n",
      "         Iterations: 53\n",
      "         Function evaluations: 110\n",
      "         Gradient evaluations: 110\n",
      "Class 3 being optimised\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.083403\n",
      "         Iterations: 25\n",
      "         Function evaluations: 55\n",
      "         Gradient evaluations: 55\n",
      "Class 4 being optimised\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.124219\n",
      "         Iterations: 42\n",
      "         Function evaluations: 92\n",
      "         Gradient evaluations: 92\n",
      "Class 5 being optimised\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.046474\n",
      "         Iterations: 29\n",
      "         Function evaluations: 59\n",
      "         Gradient evaluations: 59\n",
      "Class 6 being optimised\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.175624\n",
      "         Iterations: 43\n",
      "         Function evaluations: 82\n",
      "         Gradient evaluations: 82\n",
      "Class 7 being optimised\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.054991\n",
      "         Iterations: 22\n",
      "         Function evaluations: 43\n",
      "         Gradient evaluations: 43\n",
      "Class 8 being optimised\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.044949\n",
      "         Iterations: 29\n",
      "         Function evaluations: 63\n",
      "         Gradient evaluations: 63\n",
      "Class 9 being optimised\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.034726\n",
      "         Iterations: 40\n",
      "         Function evaluations: 77\n",
      "         Gradient evaluations: 77\n",
      "###########################################\n",
      "### Gradient Descent Optimisation finished\n",
      "###########################################\n",
      "Probability averages on train: ,85.5959595959596\n",
      "Probability averages on test: ,84.0\n",
      "precision total: 0.8384632479353457\n",
      "recall total: 0.841172763678005\n",
      "label precision recall\n",
      "label precision recall accuracy\n",
      "label precision recall accuracy\n",
      "label precision recall accuracy\n",
      "label precision recall accuracy\n",
      "label precision recall accuracy\n",
      "label precision recall accuracy\n",
      "label precision recall accuracy\n",
      "label precision recall accuracy\n",
      "label precision recall accuracy\n",
      "label precision recall accuracy\n",
      "    0     0.787  0.831\n",
      "    1     0.979  0.963\n",
      "    2     0.735  0.738\n",
      "    3     0.819  0.901\n",
      "    4     0.688  0.759\n",
      "    5     0.946  0.897\n",
      "    6     0.673  0.495\n",
      "    7     0.934  0.924\n",
      "    8     0.920  0.945\n",
      "    9     0.904  0.957\n",
      "CPU times: user 41.1 s, sys: 378 ms, total: 41.5 s\n",
      "Wall time: 21.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Declare the result arrays\n",
    "prediction_average_list = []\n",
    "prediction_average_test_list = []    \n",
    "\n",
    "data, label, data_test, label_test = read_data_in()\n",
    "\n",
    "# Declare variables for instantiation of the LogisticRegression Class\n",
    "lmbda = 1\n",
    "k = 10\n",
    "intercept = True\n",
    "max_iter = 500\n",
    "\n",
    "\n",
    "#X_test = data_test[:2000]\n",
    "\n",
    "#########################################################################################\n",
    "# Once data is read in from file data, we split it for training, validation and testing #\n",
    "# standardise, & re-shape it process                                                    #\n",
    "#########################################################################################            \n",
    "X_train,X_validate,X_test,y_train,y_validate,y_test = shape_matrix_process(split_percent=0.99)    \n",
    "\n",
    "# Saving for later final predictions\n",
    "final_data_test = X_test\n",
    "X_test = X_test[:2000]\n",
    "\n",
    "\n",
    "############################################\n",
    "# Instantiate our logistic Regression model#\n",
    "############################################\n",
    "print('#################################################')\n",
    "model = LogisticRegression(max_iter, intercept, k, lmbda)\n",
    "print('#################################################')\n",
    "\n",
    "# Carry out principal component analysis and project back with 99% of variance\n",
    "#X_train, X_reduced  = compress_project(percent=.99,data_to_compress=X_train)\n",
    "\n",
    "# Add the intercept value\n",
    "X_train,n = model.add_intercept(X_train,y_train)\n",
    "X_validate,n = model.add_intercept(X_validate,y_validate)\n",
    "\n",
    "# Create a theta matrix to capture values of theta when we minimis the objective function below\n",
    "theta = np.zeros((k,n)) #inital parameters\n",
    "\n",
    "#############################################################################################\n",
    "# Using conjugate gradient, we attempt to carry out optimisation to find theta and therefore#\n",
    "# Using theta and the data features to predict each class   #################################\n",
    "#############################################################################################\n",
    "\n",
    "# Do you want to see a verbose/full output for conjugate gradient descent?\n",
    "theta  = model.fit(X_mat=X_train,max_iter=500,y_mat=y_train,n=n, full_outp=False)\n",
    "\n",
    "#######################################################################################\n",
    "# Carry out the predictions based on the trained model and then carry out predictions #\n",
    "# on the unseen test data                                                             #\n",
    "#######################################################################################\n",
    "X_test,n = model.add_intercept(X_test,y_test)\n",
    "\n",
    "preds, prediction_average = model.predict(X=X_train,y=y_train,theta=theta)    \n",
    "prediction_average_list.append(prediction_average)\n",
    "\n",
    "preds_test,prediction_average_test = model.predict(X_test,y_test,theta=theta)    \n",
    "prediction_average_test_list.append(prediction_average_test)\n",
    "\n",
    "cm = create_confusion_matrix(y_test, preds_test)\n",
    "\n",
    "print(\"Probability averages on train: \",*prediction_average_list, sep =',')\n",
    "print(\"Probability averages on test: \",*prediction_average_test_list, sep =',')\n",
    "print(\"precision total:\", precision_average(cm))\n",
    "print(\"recall total:\", recall_average(cm))   \n",
    "\n",
    "\n",
    "print(\"label precision recall\")\n",
    "for label in range(10):\n",
    "    print(\"label precision recall accuracy\")\n",
    "for label in range(10):\n",
    "    print(f\"{label:5d} {precision(label, cm):9.3f} {recall(label, cm):6.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the confusion matrix function to create a confusion matrix\n",
    "# For either test or train\n",
    "cm = create_confusion_matrix(y_train, preds)\n",
    "#cm = create_confusion_matrix(y_test, preds_test)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(8,8)\n",
    "\n",
    "im = ax.imshow(cm, cmap='Oranges')\n",
    "\n",
    "# Take the confusion matrix array, and make it pretty with colours and text :)\n",
    "\n",
    "# A loop to go through every class and plot value for each actual vs pred class,\n",
    "# with respective colour on the Yellow Green matplotlib colour scale\n",
    "for i in range(len(np.unique(y_train))):\n",
    "    for j in range(len(np.unique(y_train))):\n",
    "        text = ax.text(j, i, cm[i, j],\n",
    "                       ha=\"center\", va=\"center\", color=\"k\")\n",
    "#plt.title(\"Confusion matrix - Test Dataset\", fontsize = 16)        \n",
    "plt.title(\"Confusion matrix - Train Dataset\", fontsize = 16)        \n",
    "\n",
    "#plt.savefig(\"ConfusionMatrix_Train.png\", dpi = 300)\n",
    "plt.savefig(\"ConfusionMatrix_Test.png\", dpi = 300)\n",
    "plt.xlabel(\"Predicted classes \", fontsize = 16)   \n",
    "plt.ylabel(\"Actual classes \", fontsize = 16)   \n",
    "fig.tight_layout()        \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_acc_metrics(cm):\n",
    "    tp = []\n",
    "    tn = []\n",
    "    fp = []\n",
    "    fn = []\n",
    "        \n",
    "    for i in range(10):\n",
    "\n",
    "        tp.append(cm[i][i])\n",
    "        fp.append(sum(cm[:,i]) - cm[i][i])\n",
    "        fn.append(sum(cm[i,:]) - cm[i][i])\n",
    "        tn.append(np.sum(cm) - (tp + fp + fn))\n",
    "\n",
    "    return(tp, fp, fn, tn)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "ACC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr[2], tpr[2], color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, output the predicted labels for the 5,000 test images we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions = model.final_predict(final_data_test, theta=theta[:,:-1])\n",
    "\n",
    "with h5py.File('predicted_labels.h5','w') as H:\n",
    "    H.create_dataset('label',data = final_predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
